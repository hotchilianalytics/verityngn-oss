# VerityNgn Environment Configuration
# Copy this file to .env and fill in your actual values

# ==============================================================================
# GOOGLE CLOUD AUTHENTICATION
# ==============================================================================

# Path to your Google Cloud service account JSON key file
# Download from: https://console.cloud.google.com/iam-admin/serviceaccounts
# Required permissions: Vertex AI User, Storage Admin (if using GCS)
GOOGLE_APPLICATION_CREDENTIALS=./verityngn/config/your-service-account-key.json

# Your Google Cloud Project ID
PROJECT_ID=your-project-id

# Google Cloud Storage bucket name (if using GCS storage)
GCS_BUCKET_NAME=your-project-id-reports

# ==============================================================================
# API KEYS
# ==============================================================================

# YouTube Data API v3 Key
# Get from: https://console.cloud.google.com/apis/credentials
# Enable: YouTube Data API v3
YOUTUBE_API_KEY=your-youtube-api-key-here

# OpenAI API Key (optional, for alternative LLM provider)
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here

# Google Custom Search API Key (for web search)
# Get from: https://console.cloud.google.com/apis/credentials
GOOGLE_SEARCH_API_KEY=your-google-search-api-key

# Google Custom Search Engine ID
# Create at: https://programmablesearchengine.google.com/
GOOGLE_SEARCH_ENGINE_ID=your-search-engine-id

# ==============================================================================
# DEPLOYMENT CONFIGURATION
# ==============================================================================

# Deployment mode: research | container | production
DEPLOYMENT_MODE=research

# Storage backend: local | gcs
STORAGE_BACKEND=local

# ==============================================================================
# LLM CONFIGURATION
# ==============================================================================

# Primary LLM model
LLM_MODEL=gemini-2.5-flash

# LLM temperature (0.0 = deterministic, 1.0 = creative)
LLM_TEMPERATURE=0.1

# Maximum output tokens
MAX_OUTPUT_TOKENS=65536

# Enable detailed LLM logging
LLM_LOGGING_ENABLED=true

# ==============================================================================
# PROCESSING CONFIGURATION
# ==============================================================================

# Maximum number of claims to extract per video
MAX_CLAIMS=40

# Minimum number of claims
MIN_CLAIMS=15

# Frames per second for video segmentation
SEGMENT_FPS=1.0

# Enable YouTube counter-intelligence search
YOUTUBE_CI_ENABLED=true

# Enable YouTube Data API
YOUTUBE_API_ENABLED=true

# ==============================================================================
# API SERVER CONFIGURATION
# ==============================================================================

# API server host
HOST=0.0.0.0

# API server port
PORT=8000

# Enable debug mode
DEBUG=false
